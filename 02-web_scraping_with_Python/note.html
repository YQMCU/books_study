<!DOCTYPE html>
<html>
<title>《用Python写网络爬虫》笔记</title>
<meta charset="utf-8">
<xmp theme="united" style="display:block;">

* 书名：用Python写网络爬虫
* 原名：Web Scraping with Python:Scrape data from any website with the power of Python
* 作者：[澳]Richard Lawson
* 译者：李斌
* 出版社：人民邮电出版社
* 版本：2016年9月第1版
* 备注：PACKT PUBLISHING
* 页面展示：[link](https://yqmcu.github.io/books_study/02-web_scraping_with_Python/note.html)


# 本书概要

* 通过跟踪链接来爬取网站
* 使用lxml从页面中抽取数据
* 构建线程爬虫
* 将下载的内容进行缓存，以降低带宽消耗
* 解析依赖于JavaScript的网站
* 与表单和会话进行交互
* 解决受保护页面的验证码问题
* 对AJAX调用进行逆向工程
* 使用Scrapy创建高级爬虫

## Chapter01 第1章 网络爬虫简介

### 1.1 网络爬虫何时有用

**重复性的手工流程**，可以利用网络爬虫技术实现**自动化处理**。
比如，了解竞争对手价格信息，或者，监测网店中心仪物品降价。

理想状态下，每个网站都应该提供API，以结构化的格式共享它们的数据。然而只是理想状态。
因此需要学习一些网络爬虫技术的相关知识，获取网站数据。

### 1.2 网络爬虫是否合法

网络爬虫目前还处于早期的蛮荒阶段，“允许哪些行为”这种基本秩序还处于建设之中。

从目前的实践来看，如果抓取数据的行为用于个人使用，则不存在问题；而如果数据用于转载，那么抓取的数据类型就非常关键了。

世界各地法院的案件告诉我们，当抓取的数据是现实生活中的**真实数据**（比如，营业地址，电话清单）时，是**允许转载**的。

但是，如果是**原创数据**（比如，意见和评论），通常就会受到版权限制，而**不能转载**。

无论如何，当你抓取某个网站的数据时，请记住自己是该网站的访客，应当约束自己的抓取行为，否则他们可能会封禁你的IP，甚至采取更进一步的法律行动。

这就要求下载请求的速度要限定在一个合理值之内，并且需要设定一个专属的用户代理来标识自己。


### 1.3 背景调研


1.3.1 检查`robots.txt`

`robots.txt`文件，可以让爬虫了解爬取该网站时存在哪些限制。

在爬取之前检查'robots.txt'文件可以最小化爬虫被封禁的可能，而且还能发现网站结构相关的线索。

[robots.txt相关](http://www.robotstxt.org)

爬虫示例`robots-demo.txt`：

    # section 1
    User-agent:BadCrawler
    Disallow:/

    # section 2
    User-agent:*
    Crawl-delay:5
    Disallow:/trap

    # section 3
    Sitemap:http://example.webscraping.con/sitemap.xml

1.3.2 检查网站地图

网站提供的`Sitemap`(网站地图)文件，可以帮助爬虫定位网站最新的内容，而无需爬取每一个网页。

了解更多[网站地图标准](http://www.sitemaps.org/protocol.html)

Sitemap可能存在问题：缺失，过期或者不完整。

1.3.3 估算网站大小

目标网站的大小会影响爬取方式。

若是几百个URL的网站，效率微不足道。但若是数百万个网页的站点，使用串行下载可能需要持续数月完成。这时就需要使用第4章中介绍的**分布式下载**来解决。

利用Google的site关键词过滤来搜索相关爬取内容的结果页。

1.3.4 识别网站所用技术

构建网站所用的技术类型也会对爬取产生影响。

基于Python的`builtwith`模块，可以检查网站构建的技术类型。

模块的安装方法：

    pip install builtwith

该模块将URL作为参数，下载该URL并对其进行分析，返回该网站使用的技术。

例子`builtwith-demo.py`：

    import builtwith
    builtwith.parse("http://example.webscraping.com")

返回结果：

    {
        u'javascript-frameworks':[u'jQuery,u'Modernizr',u'jQuery UI'],
        u'programming-languages':[u'Python],
        u'web-frameworks':[u'Web2py',u'Twitter Bootstrap'],
        u'web-servers':[u'Nginx']
    }

从结果可以看出，示例网站使用了Python的`Web2py`框架，另外使用了一些通用JavaScript库，因此该网站的内容很有可能是嵌入在HTML中，相对而言比较容易抓取。

而如果改用`AngularJS`构建该网站，此时的网站内容就很有可能是动态加载的。

如果是`ASP.NET`，那么在爬取网页时，就必须用到会话管理和表单提交了。对于这些更加复杂的情况，会在第5章和第6章中进行介绍。

1.3.5 寻找网站所有者

可以使用`WHOIS`协议查询域名的注册者是谁。

Python有个针对该协议的封装库，查看其[文档](https://pypi.python.org/pypi/python-whois)

安装：

    pip install python-whois

示例`whois-demo.py`:

    import whois
    print(whois.whois('appspot.com'))

返回结果：

    {
		...
		"name_servers":[
			"NS1.GOOGLE.COM",
			"NS2.GOOGLE.COM",
			"NS3.GOOGLE.COM",
			"NS4.GOOGLE.COM",
			"ns4.google.com",
			"ns2.google.com",
			"ns1.google.com",
			"ns3.google.com",
		],
		"org":"Google Inc.",
		"emails":[
			"abusecomplaints@markmonitor.com",
			"dns-admin@google.com"
		]
	}

1.4 编写第一个网络爬虫

为了抓取网站，我们首先需要下载包含有感兴趣数据的网页，该过程一般称为爬取(crawling)。

爬取一个网站有很多种方法，而选用哪种方法更加合适，则取决于目标网站的结构。

一般爬取网站的常见方法：

* 爬取网站地图
* 遍历每个网页的数据库ID
* 跟踪网页链接

1.4.1 下载网页

要想爬取网页，首先需要将其下载下来。
可以使用Python的`urllib2`模块下载URL

示例`url-download.py`：

    import urllib2
    def download(url):
    	print("Downloading...:",url)
    	try:
    		html = urllib2.urlopen(url).read()
    	except urllib2.URLError as e:
    		print("Download error:",e.reason)
    		html = None
    	return html

1. 重试下载

下载时遇到的错误经常是临时性的：

比如，服务器过载时返回**503 Service Unavailable** 错误。
对于此类错误，尝试**重新下载**。

如果服务器返回的是**404 Not Found** 这种错误，则说明该网页目前并不存在，再次尝试也不会有结果。


查看[HTTP错误列表](https://tools.ietf.org/html/rfc7231#section-6)

支持重新下载版本程序示例`url-download-retry.py`：

    # -*- coding: utf-8 -*-
    import urllib2
    def download(url,num_retries=2):
        print("Downloading...:",url)
        try:
            html = urllib2.urlopen(url).read()
        except urllib2.URLError as e:
            print 'Downloading error:',e.reason
            html = None
            if (num_retries > 0):
                if hasattr(e,'code') and 500 <= e.code < 600:
                    # recursively retry 5xx HTTP errors
                    return download(url,num_retries-1)
        return html

想要测试该函数可以尝试下载`http://httpstat.us/500`，始终返回500错误码。


2. 设置用户代理

默认情况下，urllib2使用`Python-urllib/2.7`作为用户代理下载网页内容，可能会被一些网站封禁。

因此，为了下载更加可靠，需要**控制用户代理的设定**。

设定用户代理的示例`url-download-set-agent`：

    # -*- coding: utf-8 -*-

    import urllib2

    def download(url,user_agent='wswp',num_retries=2):
        print("Downloading...:",url)
        headers = {'User-agent':user_agent}
        request = urllib2.Request(url,headers=headers)
        try:
            html = urllib2.urlopen(request).read()
        except urllib2.URLError as e:
            print 'Downloading error:',e.reason
            html = None
            if (num_retries > 0):
                if hasattr(e,'code') and 500 <= e.code < 600:
                    # recursively retry 5xx HTTP errors
                    return download(url,num_retries-1)
        return html

较前例相比，设定了一个默认的用户代理'wswp'(Web Scraping with Python的首字母缩写)
    
至此，download函数可以在后续实例中得到复用。该函数拥有**捕获异常**、**重试下载**、**设置用户代理**的功能。

1.4.2 网站地图爬虫

网站地图sitemap文件示例如下`sitemap.xml`：

    <?xml version="1.0" encoding="UTF-8">
    <urlset xmlns="http://www.sitemap.org/schemas/sitemap/0.9">
        <url>
            <loc>http://example.webscraping.com/view/Afghanistan-1</loc>
        </url>
        <url>
            <loc>http://example.webscraping.com/view/Aland-Islands-2</loc>
        </url>
        <url>
            <loc>http://example.webscraping.com/view/Albania-3</loc>
        </url>
    </urlset>
    </xml>

解析网站地图文件，需要使用一个简单的正则表达式，从<loc>标签中提取出URL。

示例如下`sitemap-parse.py`：    

    # -*- coding = utf-8 -*-
    from url_download import download
    import re
    def crawl_sitemap(url):
        # download the sitemap file
        sitemap = download(url)
        # extract the sitemap links
        links = re.findall('<loc>(.*?)</loc>',sitemap)
        # download each link
        for link in links:
            html = download(link)
            # scrape html here
            # ...

运行结果（无网络状态下）如下：

    ('Downloading...', 'http://localhost/books_study/02-web_scraping_with_Python/codes/sitemap.xml')
    ('Downloading...', 'http://example.webscraping.com/view/Afghanistan-1')
    ('Download error', gaierror(11004, 'getaddrinfo failed'))
    ('Downloading...', 'http://example.webscraping.com/view/Aland-Islands-2')
    ('Download error', gaierror(11004, 'getaddrinfo failed'))
    ('Downloading...', 'http://example.webscraping.com/view/Albania-3')
    ('Download error', gaierror(11004, 'getaddrinfo failed'))


1.4.3 ID遍历爬虫

由于sitemap提供的网页链接不可靠，因此需要写一个不依赖于sitemap的爬虫。

...
...

与网站地图爬虫一样，遍历ID的方法也不能保证始终有用。比如，
一些网站会检查页面别名是否满足预期，如果不是，则会返回404错误；
而另一些网站则会使用非连续大数作为ID，或者不使用数值作为ID，此时遍历就难以发挥其作用了。


1.4.2 链接爬虫

如前所述，可以通过`sitemap爬虫`和`ID遍历爬虫`最小化下载的网页数量。

不过对于另一些网站，需要让爬虫表现得更像普通用户，跟踪链接，访问感兴趣的内容。

而通过跟踪链接的方法，可以用正则表达式筛选需要下载哪些页面。

示例如下`link_scrape.py`：

    

</xmp>

<a href="https://yqmcu.github.io/books_study/01-flask_web_development/note.html">页面展示</a>


<!-- <script src="https://yqmcu.github.io/books_study/scripts/strapdown.js"></script> -->
<script src="../scripts/strapdown.js"></script>

</html> 