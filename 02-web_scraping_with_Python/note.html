<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>笔记</title>
</head>
<body>
    

<div id="md-body">
* 书名：用Python写网络爬虫
* 原名：Web Scraping with Python:Scrape data from any website with the power of Python
* 作者：[澳]Richard Lawson
* 译者：李斌
* 出版社：人民邮电出版社
* 版本：2016年9月第1版
* 备注：PACKT PUBLISHING


# 本书概要

* 通过跟踪链接来爬取网站
* 使用lxml从页面中抽取数据
* 构建线程爬虫
* 将下载的内容进行缓存，以降低带宽消耗
* 解析依赖于JavaScript的网站
* 与表单和会话进行交互
* 解决受保护页面的验证码问题
* 对AJAX调用进行逆向工程
* 使用Scrapy创建高级爬虫

## Chapter01 第1章 网络爬虫简介

### 1.1 网络爬虫何时有用

**重复性的手工流程**，可以利用网络爬虫技术实现**自动化处理**。
比如，了解竞争对手价格信息，或者，监测网店中心仪物品降价。

理想状态下，每个网站都应该提供API，以结构化的格式共享它们的数据。然而只是理想状态。
因此需要学习一些网络爬虫技术的相关知识，获取网站数据。

### 1.2 网络爬虫是否合法

网络爬虫目前还处于早期的蛮荒阶段，“允许哪些行为”这种基本秩序还处于建设之中。

从目前的实践来看，如果抓取数据的行为用于个人使用，则不存在问题；而如果数据用于转载，那么抓取的数据类型就非常关键了。

世界各地法院的案件告诉我们，当抓取的数据是现实生活中的**真实数据**（比如，营业地址，电话清单）时，是**允许转载**的。

但是，如果是**原创数据**（比如，意见和评论），通常就会受到版权限制，而**不能转载**。

无论如何，当你抓取某个网站的数据时，请记住自己是该网站的访客，应当约束自己的抓取行为，否则他们可能会封禁你的IP，甚至采取更进一步的法律行动。

这就要求下载请求的速度要限定在一个合理值之内，并且需要设定一个专属的用户代理来标识自己。


### 1.3 背景调研


#### 1.3.1 检查`robots.txt`

`robots.txt`文件，可以让爬虫了解爬取该网站时存在哪些限制。

在爬取之前检查'robots.txt'文件可以最小化爬虫被封禁的可能，而且还能发现网站结构相关的线索。

[robots.txt相关](http://www.robotstxt.org)

爬虫示例`robots_demo.txt`：

    # section 1
    User-agent:BadCrawler
    Disallow:/

    # section 2
    User-agent:*
    Crawl-delay:5
    Disallow:/trap

    # section 3
    Sitemap:http://example.webscraping.con/sitemap.xml

#### 1.3.2 检查网站地图

网站提供的`Sitemap`(网站地图)文件，可以帮助爬虫定位网站最新的内容，而无需爬取每一个网页。

了解更多[网站地图标准](http://www.sitemaps.org/protocol.html)

Sitemap可能存在问题：缺失，过期或者不完整。

#### 1.3.3 估算网站大小

目标网站的大小会影响爬取方式。

若是几百个URL的网站，效率微不足道。但若是数百万个网页的站点，使用串行下载可能需要持续数月完成。这时就需要使用第4章中介绍的**分布式下载**来解决。

利用Google的site关键词过滤来搜索相关爬取内容的结果页。

#### 1.3.4 识别网站所用技术

构建网站所用的技术类型也会对爬取产生影响。

基于Python的`builtwith`模块，可以检查网站构建的技术类型。

模块的安装方法：

    pip install builtwith

该模块将URL作为参数，下载该URL并对其进行分析，返回该网站使用的技术。

例子`builtwith_demo.py`：

    import builtwith
    builtwith.parse("http://example.webscraping.com")

返回结果：

    {
        u'javascript-frameworks':[u'jQuery,u'Modernizr',u'jQuery UI'],
        u'programming-languages':[u'Python],
        u'web-frameworks':[u'Web2py',u'Twitter Bootstrap'],
        u'web-servers':[u'Nginx']
    }

从结果可以看出，示例网站使用了Python的`Web2py`框架，另外使用了一些通用JavaScript库，因此该网站的内容很有可能是嵌入在HTML中，相对而言比较容易抓取。

而如果改用`AngularJS`构建该网站，此时的网站内容就很有可能是动态加载的。

如果是`ASP.NET`，那么在爬取网页时，就必须用到会话管理和表单提交了。对于这些更加复杂的情况，会在第5章和第6章中进行介绍。

#### 1.3.5 寻找网站所有者

可以使用`WHOIS`协议查询域名的注册者是谁。

Python有个针对该协议的封装库，查看其[文档](https://pypi.python.org/pypi/python-whois)

安装：

    pip install python-whois

示例`whois_demo.py`:

    import whois
    print(whois.whois('appspot.com'))

返回结果：

    {
        ...
        "name_servers":[
            "NS1.GOOGLE.COM",
            "NS2.GOOGLE.COM",
            "NS3.GOOGLE.COM",
            "NS4.GOOGLE.COM",
            "ns4.google.com",
            "ns2.google.com",
            "ns1.google.com",
            "ns3.google.com",
        ],
        "org":"Google Inc.",
        "emails":[
            "abusecomplaints@markmonitor.com",
            "dns-admin@google.com"
        ]
    }

### 1.4 编写第一个网络爬虫

为了抓取网站，我们首先需要下载包含有感兴趣数据的网页，该过程一般称为爬取(crawling)。

爬取一个网站有很多种方法，而选用哪种方法更加合适，则取决于目标网站的结构。

一般爬取网站的常见方法：

* 爬取网站地图
* 遍历每个网页的数据库ID
* 跟踪网页链接

#### 1.4.1 下载网页

要想爬取网页，首先需要将其下载下来。
可以使用Python的`urllib2`模块下载URL

示例`url_download.py`：

    import urllib2
    def download(url):
        print("Downloading...:",url)
        try:
            html = urllib2.urlopen(url).read()
        except urllib2.URLError as e:
            print("Download error:",e.reason)
            html = None
        return html

1. 重试下载

下载时遇到的错误经常是临时性的：

比如，服务器过载时返回**503 Service Unavailable** 错误。
对于此类错误，尝试**重新下载**。

如果服务器返回的是**404 Not Found** 这种错误，则说明该网页目前并不存在，再次尝试也不会有结果。


查看[HTTP错误列表](https://tools.ietf.org/html/rfc7231#section-6)

支持重新下载版本程序示例`url_download_retry.py`：

    # -*- coding: utf-8 -*-
    import urllib2
    def download(url,num_retries=2):
        print("Downloading...:",url)
        try:
            html = urllib2.urlopen(url).read()
        except urllib2.URLError as e:
            print 'Downloading error:',e.reason
            html = None
            if (num_retries > 0):
                if hasattr(e,'code') and 500 <= e.code < 600:
                    # recursively retry 5xx HTTP errors
                    return download(url,num_retries-1)
        return html

想要测试该函数可以尝试下载`http://httpstat.us/500`，始终返回500错误码。


2. 设置用户代理

默认情况下，urllib2使用`Python-urllib/2.7`作为用户代理下载网页内容，可能会被一些网站封禁。

因此，为了下载更加可靠，需要**控制用户代理的设定**。

设定用户代理的示例`url_download_set_agent`：

    # -*- coding: utf-8 -*-

    import urllib2

    def download(url,user_agent='wswp',num_retries=2):
        print("Downloading...:",url)
        headers = {'User-agent':user_agent}
        request = urllib2.Request(url,headers=headers)
        try:
            html = urllib2.urlopen(request).read()
        except urllib2.URLError as e:
            print 'Downloading error:',e.reason
            html = None
            if (num_retries > 0):
                if hasattr(e,'code') and 500 <= e.code < 600:
                    # recursively retry 5xx HTTP errors
                    return download(url,num_retries-1)
        return html

较前例相比，设定了一个默认的用户代理'wswp'(Web Scraping with Python的首字母缩写)
    
至此，download函数可以在后续实例中得到复用。该函数拥有**捕获异常**、**重试下载**、**设置用户代理**的功能。

#### 1.4.2 网站地图爬虫

网站地图sitemap文件示例如下`sitemap.xml`：

    <?xml version="1.0" encoding="UTF-8">
    <urlset xmlns="http://www.sitemap.org/schemas/sitemap/0.9">
        <url>
            <loc>http://example.webscraping.com/view/Afghanistan-1</loc>
        </url>
        <url>
            <loc>http://example.webscraping.com/view/Aland-Islands-2</loc>
        </url>
        <url>
            <loc>http://example.webscraping.com/view/Albania-3</loc>
        </url>
    </urlset>
    </xml>

解析网站地图文件，需要使用一个简单的正则表达式，从<loc>标签中提取出URL。

示例如下`sitemap_parse.py`：    

    # -*- coding = utf-8 -*-
    from url_download import download
    import re
    def crawl_sitemap(url):
        # download the sitemap file
        sitemap = download(url)
        # extract the sitemap links
        links = re.findall('<loc>(.*?)</loc>',sitemap)
        # download each link
        for link in links:
            html = download(link)
            # scrape html here
            # ...

运行结果（无网络状态下）如下：

    ('Downloading...', 'http://localhost/books_study/02-web_scraping_with_Python/codes/sitemap.xml')
    ('Downloading...', 'http://example.webscraping.com/view/Afghanistan-1')
    ('Download error', gaierror(11004, 'getaddrinfo failed'))
    ('Downloading...', 'http://example.webscraping.com/view/Aland-Islands-2')
    ('Download error', gaierror(11004, 'getaddrinfo failed'))
    ('Downloading...', 'http://example.webscraping.com/view/Albania-3')
    ('Download error', gaierror(11004, 'getaddrinfo failed'))


#### 1.4.3 ID遍历爬虫

由于sitemap提供的网页链接不可靠，因此需要写一个不依赖于sitemap的爬虫。

...
...

与网站地图爬虫一样，遍历ID的方法也不能保证始终有用。比如，
一些网站会检查页面别名是否满足预期，如果不是，则会返回404错误；
而另一些网站则会使用非连续大数作为ID，或者不使用数值作为ID，此时遍历就难以发挥其作用了。


#### 1.4.2 链接爬虫

如前所述，可以通过`sitemap爬虫`和`ID遍历爬虫`最小化下载的网页数量。

不过对于另一些网站，需要让爬虫表现得更像普通用户，跟踪链接，访问感兴趣的内容。

而通过跟踪链接的方法，可以用正则表达式筛选需要下载哪些页面。

示例如下`link_scrape.py`：


## Chapter02 第2章 数据抓取

在上一章中，构建了一个爬虫，可以通过跟踪链接的方式下载所需的网页，虽然可行，但是不够实用，因为爬虫在下载网页之后又将结果丢弃掉了。
现在，我们需要让这个爬虫从每个网页中抽取一些数据，然后实现某些事情，这种做法称为**抓取**(scraping)。

抽取网页数据的方法有三种：

* 正则表达式
* Beautiful Soup
* lxml

### 2.1 分析网页

查看源代码是了解一个网页的结构的一种实用方法。

**笔者注**：笔者实用的Chrome内核浏览器，F12调用开发者工具可查看整个网页的结构,并且可以debug。

### 2.2 三种网页抓取方法

#### 2.2.1 

Python正则表达式相关知识可以查看[完整介绍](https://docs.python.org/2/howto/regex.html)


正则表达式抓取表格数据示例如下`regex_demo.py`:

    # -*- coding : utf-8 -*-
    from url_download import download
    import re
    url = "http://example.webscraping.com/view/United-Kingdom-239"
    html = download(url)
    re.findall('<td class="w2p_fw"(.*?)</td>',html)

正则表达式弊端：正则表达式存在**难以构造**、**可读性差**的问题。此外，还有网页更新后会使该正则表达式方法无法满足抓取需求。

#### 2.2.2 Beautiful Soup

`Beautiful Soup`是一个Python模块，该模块可以解析网页，并提供定位内容的便捷接口。

模块的安装方法：

    pip install beautifulsoup4

使用`Beautiful Soup`模块的第一步是**将已下载HTML内容解析为soup文档**。（格式化HTML）

示例`soup_parse_demo.py`如下：    

    # -*- coding = utf-8 -*-
    from bs4 import BeautifulSoup
    broken_html = '<ul class="country"><li>Area<li>Population</ul>'
    # parse the HTML
    soup = BeautifulSoup(broken_html,"html.parse")
    fixed_html = soup.prettify()
    print fixed_html

返回结果：

    <ul class="country">
        <li>Area</li>
        <li>Population</li>
    </ul>

从执行结果中可以看出`Beautiful Soup`能够正确解析确实的引号并闭合标签。
然后使用`find()`，和`findall()`方法来定位需要的元素。


`find()`示例`bs4_find_demo.py`如下：

    # -*- coding = utf-8 -*-
    from bs4 import BeautifulSoup
    broken_html = '<ul class="country"><li>Area<li>Population</ul>'
    # parse the HTML
    soup = BeautifulSoup(broken_html,"html.parse")
    ul = soup.find('ul',attrs={'class':'country'})
    ul.find('li') # returns just the first match
    ur.find_all('li') # returns all matches

返回结果：

    [<li>Area</li>,<li>Population</li>]

查看全部BeautifulSoup的[官方文档](http://www.crummy.com/software/BeautifulSoup/bs4/doc)

完整代码`bs4_scraping.py`示例：

    from url_download import download
    from bs4 import BeautifulSoup
    url = "http://example.webscraping.com/places/view/United-Kingdom-239"
    html = download(url)
    soup = BeautifulSoup(html,"lxml")
    # locate the area row
    tr = soup.find(attrs={'id':'places_area__row'})
    td = tr.find(attrs={'class':'w2p_fw'}) # locate the area tag
    area = td.text # extract the text from this tag
    print area

返回结果：
    
    244,820 square kilometres

**笔者注**：`BeautifulSoup(html,"lxml")`被修改

使用BeautifulSoup更容易构造和理解程序。而且布局的小变化不会对程序造成很大影响。


#### 2.2.3 Lxml

Lxml是基于`libxml2`这一XML解析库的Python封装。该模块使用C语言编写，解析速度比`Beautiful Soup`更快。

模块的安装方法：

    pip install lxml

安装成功：

    Installing collected packages: lxml
    Successfully installed lxml-3.7.3

最新安装说明可以参考[http://Lxml.de/installation.html](http://Lxml.de/installation.html)

lxml格式化HTML示例`lxml_parse_demo.py`如下：

    # -*- coding = utf-8 -*-
    import lxml.html
    broken_html = '<ul class=country><li>Area<li>Population</ul>'
    tree = lxml.html.fromstring(broken_html) # parse the HTML
    fixed_html = lxml.html.tostring(tree,pretty_print=True)
    print(fixed_html)

返回结果：

    <ul class="country">
    <li>Area</li>
    <li>Population</li>
    </ul>

解析完输入内容之后，进入选择元素的步骤，此时`lxml`有几种不同的方法，比如`XPath选择器`和类似`Beautiful Soup`的`find()`方法。

不过后续我们将会使用`CSS选择器`，它更加简洁，并且能够在第5章解析动态内容时得以复用。

使用lxml的CSS选择器抽取数据的示例`lxml_css_demo.py`如下：

    # -*- coding = utf-8 -*-
    from url_download import download
    import lxml.html
    url = "http://example.webscraping.com/places/view/United-Kingdom-239"
    html = download(url)
    tree = lxml.html.fromstring(html) # parse the HTML
    td = tree.cssselect('tr#places_area__row > td.w2p_fw')[0]
    area = td.text_content()
    print(area)

返回结果：
    
    244,820 square kilometres

**笔者注**：使用`cssselect()`方法需要安装cssselect模块

安装css选择器的方法:

    pip install cssselect

安装成功：

    Installing collected packages: cssselect
    Successfully installed cssselect-1.0.1

此例中的css选择器代码：`tree.cssselect('tr#places_area__row > td.w2p_fw')[0]`，选择了id为places_area__row的行标签的class为w2p_fw的子标签。

**CSS选择器的一些常用示例如下：**

* 选择所有标签：`*`
* 选择&lt;a&gt;标签：`a`
* 选择所有class="link"的元素：`.link`
* 选择class="link"的&lt;a&gt;标签：`a.link`
* 选择id="home"的元素：`#home`
* 选择id="home"的&lt;a&gt;标签：`a#home`
* 选择父元素为&lt;a&gt;标签的所有&lt;span&gt;子标签：a > span
* 选择&lt;a&gt;标签内部的所有&lt;span&gt;标签：`a span`
* 选择title属性为"Home"的所有&lt;a&gt;标签：`a[title=Home]`

关于W3C提出的CSS3规范，其网址为[http://www.w3.org/TR/2011/REC-css3-selectors-20110929](http://www.w3.org/TR/2011/REC-css3-selectors-20110929).

Lxml已经实现了大部分CSS3属性，其支持的功能参见[https://cssselect.readthedocs.io/en/latest/#supported-selectors](https://cssselect.readthedocs.io/en/latest/#supported-selectors)

**笔者注**：cssselect文档URL已经变更。

需要注意的是，lxml的内部实现中，实际上是**将CSS选择器转换为等价的XPath转换器**。

#### 2.2.4 性能对比

...
...

实验结果：

* Beautiful Soup最慢。

因为lxml和正则表达式模块都是C语言编写的，而BeautifulSoup是纯Python编写的。

* lxml表现得和正则表达式差不多好。

由于lxml在搜索元素之前，必须将输入解析为内部格式，因此会产生额外的开销。
而当抓取同一网页的多个特征时，这种解析产生的开销就会降低，**lxml**也就**更具有竞争力**。


#### 2.2.5 结论

每种抓取方法的优缺点：

|抓取方法|性能|使用难度|安装难度|
|----|----|----|----
|正则表达式|快|困难|简单（内置模块）|
|Beautiful Soup|慢|简单|简单（纯Python）|
|Lxml|快|简单|简单（C语言）|




</div>
    
<script src="../scripts/markdown_parse.js"></script>


</body>
</html>